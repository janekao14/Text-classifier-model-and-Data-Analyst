{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiat Brand Image\n",
    "1. Data Collection\n",
    "\n",
    "1.1 general ev\n",
    "\n",
    "1.2 fiat 500\n",
    "\n",
    "1.3 audi_a3_etron\n",
    "\n",
    "1.4 general charging\n",
    "\n",
    "1.5 renault zoe\n",
    "\n",
    "1.6 peugeot e208\n",
    "\n",
    "1.7 citroen ec4\n",
    "\n",
    "1.8 mini\n",
    "\n",
    "1.9 bmw\n",
    "\n",
    "1.10 bmw i3\n",
    "\n",
    "1.11 fiat\n",
    "\n",
    "1.12 fiat ampera volt\n",
    "\n",
    "1.13 gm\n",
    "\n",
    "1.14 honda e \n",
    "\n",
    "1.15 hyundai \n",
    "\n",
    "1.16 hyundai jona\n",
    "\n",
    "1.17 kia niro\n",
    "\n",
    "1.18 kia soul\n",
    "\n",
    "1.19 kia ev4\n",
    "\n",
    "1.20 kia\n",
    "\n",
    "1.21 smart ev\n",
    "\n",
    "1.22 renault\n",
    "\n",
    "1.23 corsa e\n",
    "\n",
    "1.24 mitsubishi\n",
    "\n",
    "1.25 nissian\n",
    "\n",
    "1.26 nissian ariya \n",
    "\n",
    "1.27 nissian leaf gen 1\n",
    "\n",
    "1.28 nissian leag gen 2\n",
    "\n",
    "1.29 volkswagen passat gte\n",
    "\n",
    "1.30 volkswagen egolf\n",
    "\n",
    "1.31 volkswagen golf gte\n",
    "\n",
    "1.32 volkswagen id3\n",
    "\n",
    "1.33 volkswagen eup\n",
    "\n",
    "1.34 volkswagen\n",
    "\n",
    "1.35 skoda\n",
    "\n",
    "1.36 audi\n",
    "\n",
    "1.37 combine all datasets\n",
    "\n",
    "2. label sample data\n",
    "\n",
    "\n",
    "2.1 apply code to label sample data\n",
    "\n",
    "2.2 label data manually (no code)\n",
    "\n",
    "3. Model Building\n",
    "\n",
    "3.1 combine datasets from 5 members\n",
    "\n",
    "3.2 text preprocessing\n",
    "\n",
    "3.3 train models\n",
    "\n",
    "3.4 apply trained model to the entire dataset\n",
    "\n",
    "4. Sentiment Analysis\n",
    "\n",
    "\n",
    "5. Topic Modelling\n",
    "\n",
    "5.1 inital attempt\n",
    "\n",
    "5.2 adjust number of topics : Fiat 11 & competitors 13\n",
    "\n",
    "5.3 explore topic 2 in fiat and topic 10 in competitors since they are dominant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 general ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general ev page 1-51\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 51)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 52-77\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(52, 77)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 78-102\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(78, 102)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_2_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 103-130\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(103, 130)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 131-154\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(131, 154)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_3_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 155-178\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(155, 178)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 179-206\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(179, 206)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_4_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 207-260\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(207, 260)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('general_ev_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "files = [\n",
    "    'general_ev_1.csv',\n",
    "    'general_ev_2.csv',\n",
    "    'general_ev_3.csv',\n",
    "    'general_ev_3_1.csv',\n",
    "    'general_ev_4.csv',\n",
    "    'general_ev_4_1.csv',\n",
    "    'general_ev_5.csv'\n",
    "]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df_list = [pd.read_csv(file) for file in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('general_ev.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 fiat 500e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/fiat-500.361\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/fiat-500.361/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 3)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('fiat500.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 audi_a3_etron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/a3-etron/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/a3-etron/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 15)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('audi_a3_etron.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 general charging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 1-51\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 51)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 52-77\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-discussion/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(52, 77)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 78-102\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(78, 102)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_2_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 103-130\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(103, 130)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 131-154\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(131, 154)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_3_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 155-178\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(155, 178)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 179-206\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(179, 206)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_4_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 107-260\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ev-charging/\")\n",
    "\n",
    "# Function to handle pop-up\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    page_count = start_page\n",
    "    base_url = \"https://www.speakev.com/forums/ev-charging/page-{}\"\n",
    "\n",
    "    while page_count <= end_page:\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "        page_count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(207, 260)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('charging_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "files = [\n",
    "    'charging_1.csv',\n",
    "    'charging_2.csv',\n",
    "    'charging_3.csv',\n",
    "    'charging_3_1.csv',\n",
    "    'charging_4.csv',\n",
    "    'charging_4_1.csv',\n",
    "    'charging_5.csv'\n",
    "]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df_list = [pd.read_csv(file) for file in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('charging.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 renault_zoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 1-63\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/renault-zoe/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/renault-zoe/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 63)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('renault_zoe_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 64-126\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/renault-zoe/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/renault-zoe/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(64, 126)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('renault_zoe_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "files = [\n",
    "    'renault_zoe_1.csv',\n",
    "    'renault_zoe_2.csv'\n",
    "    \n",
    "]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df_list = [pd.read_csv(file) for file in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('renault_zoe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 peugeot_e208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/peugeot-e208.371/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/peugeot-e208.371/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 10)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('peugeot_e208.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 citreon_ec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/citroen-ec4.400/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/citroen-ec4.400/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 3)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('citroen_ec4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/mini.300/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/mini.300/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 4)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('mini.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 bmw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/bmw-i/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/bmw-i/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 3)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('bmw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 bmw_i3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 1-31\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/bmw-i3/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/bmw-i3/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 31)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('bmw_i3_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 32-63\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/bmw-i3/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/bmw-i3/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(32, 63)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('bmw_i3_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('bmw_i3_1.csv')\n",
    "df2 = pd.read_csv('bmw_i3_2.csv')\n",
    "\n",
    "# Combine the two dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_file_path = 'bmw_i3.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11 fiat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/general-fiat-discussion.360/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/general-fiat-discussion.360/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 1)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('fiat.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.12 fiat ampera volt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 1-43\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ampera-volt/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/ampera-volt/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 43)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('fiat_ampera_volt_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 44-86\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/ampera-volt/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/ampera-volt/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(44, 86)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('fiat_ampera_volt_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('fiat_ampera_volt_1.csv')\n",
    "df2 = pd.read_csv('fiat_ampera_volt_2.csv')\n",
    "\n",
    "# Combine the two dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_file_path = 'fiat_ampera_volt_combined.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.13 gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/gm-ev-discussion/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/gm-ev-discussion/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 5)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('gm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.14 honda e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/honda-e.352/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/honda-e.352/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 1)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('honda_e.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.15 hyundai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/general-hyundai-ev-discussion.169/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/general-hyundai-ev-discussion.169/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 4)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('hyundai.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.16 hyundai_kona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/hyundai-kona.281/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/hyundai-kona.281/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 59)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('hyundai_kona.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.17 kia niro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/kia-niro.298/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/kia-niro.298/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 66)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('kia_niro.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.18 kia soul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/soul-ev/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/soul-ev/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 28)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('kia_soul.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.19 kia ev6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/kia-ev6.395/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/kia-ev6.395/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 17)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('kia_ev6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.20 kia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/general-kia-ev-discussion.148/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/general-kia-ev-discussion.148/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 8)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('kia.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.21 smart ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/smart-ev-forum.428/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/smart-ev-forum.428/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 1)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('smart_ev.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.22 renault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/renault-evs/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/renault-evs/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 11)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('renault.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.23 corsa e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/corsa-e.373/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/corsa-e.373/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 10)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('corsa_e.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.24 mitsubishi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/mitsubishi-ev-gen/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/mitsubishi-ev-gen/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 5)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('mitsubishi.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.25 nissian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/nissan-evs/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/nissan-evs/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 25)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('nissian.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.26 nissian ariya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/nissan-ariya.379/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/nissan-ariya.379/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 4)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('nissian_ariya.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.27 nissian leaf generation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 1-49\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/first-gen-nissan-leaf-leaf24-leaf30.130/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/first-gen-nissan-leaf-leaf24-leaf30.130/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 49)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('nissian_leaf_1_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 50-99\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/first-gen-nissan-leaf-leaf24-leaf30.130/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/first-gen-nissan-leaf-leaf24-leaf30.130/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(50, 99)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('nissian_leaf_1_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 100-148\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/first-gen-nissan-leaf-leaf24-leaf30.130/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/first-gen-nissan-leaf-leaf24-leaf30.130/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(100, 148)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('nissian_leaf_1_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "files = [\n",
    "    'nissian_leaf_1_1.csv',\n",
    "    'nissian_leaf_1_2.csv',\n",
    "    'nissian_leaf_1_3.csv'\n",
    "]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df_list = [pd.read_csv(file) for file in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('nissian_leaf_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.28 nissian leaf generation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/second-gen-nissan-leaf-leaf40-leaf62.273/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/second-gen-nissan-leaf-leaf40-leaf62.273/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 40)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('nissian_leaf_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.29 volkswagen passat gte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/volkswagen-passat-gte.153/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/volkswagen-passat-gte.153/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 17)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('volkswagen_passat_gte.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.30 volkswagen egolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/vw-egolf/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/vw-egolf/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 28)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('volkswagen_egolf.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.31 volkswagen golf gte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/vw-golf-GTE/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/vw-golf-GTE/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 64)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('volkswagen_golf_gte.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.32 volkswagen id3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/volkswagen-id-3.316/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/volkswagen-id-3.316/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 36)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('volkswagen_id3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.33 volkswagen eup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/vw-eup/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/vw-eup/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 6)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('volkswagen_eup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.34 volkswagen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/vw-evs/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/vw-evs/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 10)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('volkswagen.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.35 skoda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/general-skoda-ev-discussions.344/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/general-skoda-ev-discussions.344/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 4)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('skoda.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.36 audi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome('/Users/janekao/Desktop/chromedriver-mac-arm64/chromedriver')  # Update this path\n",
    "driver.get(\"https://www.speakev.com/forums/general-audi-ev-discussion.135/\")\n",
    "\n",
    "# Function to handle pop-up (if any)\n",
    "def handle_popup():\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"css-47sehv\"))).click()\n",
    "        print(\"Popup dismissed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Popup handling error: {e}\")\n",
    "\n",
    "handle_popup()\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scrape forum pages\n",
    "def scrape_forum_pages(start_page, end_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.speakev.com/forums/general-audi-ev-discussion.135/page-{}\"\n",
    "\n",
    "    for page_count in range(start_page, end_page + 1):\n",
    "        driver.get(base_url.format(page_count))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")))\n",
    "        topics = driver.find_elements(By.CSS_SELECTOR, \"a[data-xf-init='preview-tooltip']\")\n",
    "        for topic in topics:\n",
    "            data.append({\"title\": topic.text.strip(), \"link\": topic.get_attribute('href')})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape topics from specified pages\n",
    "topic_details = scrape_forum_pages(1, 3)\n",
    "\n",
    "# Function to scrape discussion details\n",
    "def scrape_discussion(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.message-body\")))\n",
    "    posts = driver.find_elements(By.CSS_SELECTOR, \"article.message-body\")\n",
    "    authors = driver.find_elements(By.CSS_SELECTOR, \"a.username\")\n",
    "    dates = driver.find_elements(By.CSS_SELECTOR, \"time\")\n",
    "\n",
    "    discussion_data = []\n",
    "    # Loop through all posts and replies\n",
    "    for index, post in enumerate(posts):\n",
    "        author = authors[index].text if index < len(authors) else \"Unknown\"\n",
    "        date = dates[index].get_attribute('datetime') if index < len(dates) else \"Unknown Date\"\n",
    "        content = post.text\n",
    "\n",
    "        discussion_data.append({\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return discussion_data\n",
    "\n",
    "# Collect data from discussions\n",
    "all_data = []\n",
    "for detail in topic_details:\n",
    "    discussions = scrape_discussion(detail['link'])\n",
    "    for disc in discussions:\n",
    "        all_data.append({\n",
    "            \"topic\": detail['title'],\n",
    "            \"link\": detail['link'],\n",
    "            **disc\n",
    "        })\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('audi.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.37 combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('general_ev.csv')\n",
    "df2 = pd.read_csv('fiat500.csv')\n",
    "df3 = pd.read_csv('audi_a3_etron.csv')\n",
    "df4 = pd.read_csv('charging.csv')\n",
    "df5 = pd.read_csv('renault_zoe.csv')\n",
    "df6 = pd.read_csv('peugeot_e208.csv')\n",
    "df7 = pd.read_csv('citroen_ec4.csv')\n",
    "df8 = pd.read_csv('mini.csv')\n",
    "df9 = pd.read_csv('bmw.csv')\n",
    "df10 = pd.read_csv('bmw_i3.csv')\n",
    "df11 = pd.read_csv('fiat.csv')\n",
    "df12 = pd.read_csv('fiat_ampera_volt_combined.csv')\n",
    "df13 = pd.read_csv('gm.csv')\n",
    "df14 = pd.read_csv('honda_e.csv')\n",
    "df15 = pd.read_csv('hyundai.csv')\n",
    "df16 = pd.read_csv('hyundai_kona.csv')\n",
    "df17 = pd.read_csv('kia_niro.csv')\n",
    "df18 = pd.read_csv('kia_soul.csv')\n",
    "df19 = pd.read_csv('kia_ev6.csv')\n",
    "df20 = pd.read_csv('kia.csv')\n",
    "df21 = pd.read_csv('smart_ev.csv')\n",
    "df22 = pd.read_csv('renault.csv')\n",
    "df23 = pd.read_csv('corsa_e.csv')\n",
    "df24 = pd.read_csv('mitsubishi.csv')\n",
    "df25 = pd.read_csv('nissian.csv')\n",
    "df26 = pd.read_csv('nissian_ariya.csv')\n",
    "df27 = pd.read_csv('nissian_leaf_1.csv')\n",
    "df28 = pd.read_csv('nissian_leaf_2.csv')\n",
    "df29 = pd.read_csv('volkswagen_passat_gte.csv')\n",
    "df30 = pd.read_csv('volkswagen_egolf.csv')\n",
    "df31 = pd.read_csv('volkswagen_golf_gte.csv')\n",
    "df32 = pd.read_csv('volkswagen_id3.csv')\n",
    "df33 = pd.read_csv('volkswagen_eup.csv')\n",
    "df34 = pd.read_csv('volkswagen.csv')\n",
    "df35 = pd.read_csv('skoda.csv')\n",
    "df36 = pd.read_csv('audi.csv')\n",
    "\n",
    "# Assuming you have multiple DataFrames: df1, df2, df3\n",
    "combined_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35, df36], ignore_index=True)\n",
    "\n",
    "# drop columns\n",
    "combined_df = combined_df.drop(columns=['topic', 'link'])\n",
    "\n",
    "# Convert the 'date' column to datetime format with UTC handling\n",
    "combined_df_new['date'] = pd.to_datetime(combined_df_new['date'], format='%Y-%m-%dT%H:%M:%S%z', utc=True, errors='coerce')\n",
    "\n",
    "# Now convert the datetime to 'YYYY/MM/DD'\n",
    "combined_df_new['date'] = combined_df_new['date'].dt.strftime('%Y/%m/%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. label sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 apply code to label sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df1 = pd.read_excel('1500_sample.xlsx')\n",
    "\n",
    "# Replace NaN values\n",
    "df1['Replies'] = df1['Replies'].fillna('')\n",
    "\n",
    "# Define the keywords for each category\n",
    "electric_fit_keywords = [\"battery\", \"charged\", \"charge\", \"charging\", \"software\", \"performance\", \"quality\", \"technical\", \"drive\", \"driving\", \"experience\", \"power\", \"torque\", \"autonomous\", \"range\", \"miles\"]\n",
    "affordability_keywords = [\"cost\", \"price\", \"pricing\", \"expensive\", \"cheap\", \"maintenance\", \"repair\", \"expenses\", \"charge cost\", \"£\", \"$\", \"€\", \"money\", \"value\"]\n",
    "customer_care_keywords = [\"dealer\", \"dealership\", \"purchase\", \"buy\", \"after-sales\", \"support\", \"service\", \"customer service\", \"satisfaction\", \"repair\", \"warranty\", \"contact\", \"helpful\", \"unhelpful\"]\n",
    "\n",
    "# Function to label the replies\n",
    "def label_reply_multiple(reply, keywords):\n",
    "    reply = reply.lower()\n",
    "    return any(keyword in reply for keyword in keywords)\n",
    "\n",
    "# Apply the labeling function for each category\n",
    "df1['Electric Fit'] = df1['Replies'].apply(lambda x: 1 if label_reply_multiple(x, electric_fit_keywords) else 0)\n",
    "df1['Affordability'] = df1['Replies'].apply(lambda x: 1 if label_reply_multiple(x, affordability_keywords) else 0)\n",
    "df1['Customer Care'] = df1['Replies'].apply(lambda x: 1 if label_reply_multiple(x, customer_care_keywords) else 0)\n",
    "\n",
    "# Save the labeled data to a new Excel file\n",
    "output_file_path = 'multi_labelled.xlsx'\n",
    "df1.to_excel(output_file_path, index=False)\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 label data manually (no code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result of keyword labelling was not satisfactroy, the group decided to hand label the sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Combine datasets from all 5 members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('cartalk.csv')\n",
    "df2 = pd.read_csv('speakev.csv')\n",
    "df3 = pd.read_csv('pistonhead.csv')\n",
    "df4 = pd.read_csv('insideevs.csv')\n",
    "df5 = pd.read_csv('edmunds.csv')\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('main.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load necessary resources\n",
    "nlp = spacy.load('/Users/janekao/Desktop/en_core_web_sm-3.1.0/en_core_web_sm/en_core_web_sm-3.1.0')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing functions\n",
    "def text_clean(raw_text):\n",
    "    txt1 = str(raw_text).lower().strip()\n",
    "    txt2 = re.sub(\"[^a-zA-Z]\", \" \", txt1).split()\n",
    "    return txt2\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        yield(text_clean(sent))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=1, threshold=10)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization_str(texts):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(' '.join([token.lemma_ for token in doc]))\n",
    "    return texts_out\n",
    "\n",
    "# Function to clean a dataset\n",
    "def clean_dataset(df, text_column):\n",
    "    text_feed = df[text_column].tolist()\n",
    "    \n",
    "    print('Remove non-alphabetic characters and convert to lower case...')\n",
    "    text_nonum = list(sent_to_words(text_feed))\n",
    "\n",
    "    print('Remove stop words...')\n",
    "    text_nostops = remove_stopwords(text_nonum)\n",
    "\n",
    "    print('Form bigrams...')\n",
    "    text_bigrams = make_bigrams(text_nostops)\n",
    "\n",
    "    print('Lemmatize words...')\n",
    "    text_lemmatized = lemmatization_str(text_bigrams)\n",
    "    \n",
    "    return text_lemmatized\n",
    "\n",
    "# Load and clean sample data\n",
    "sample_df = pd.read_csv('/Users/janekao/Desktop/selected_label_samples_withothers.csv')\n",
    "text_prep_sample = clean_dataset(sample_df, 'Review')\n",
    "\n",
    "# Load and clean main data\n",
    "main_df = pd.read_csv('/Users/janekao/Desktop/filtered_main.csv')\n",
    "text_prep_main = clean_dataset(main_df, 'Review')\n",
    "\n",
    "# Save cleaned datasets to CSV\n",
    "sample_df['Cleaned_Review'] = text_prep_sample\n",
    "main_df['Cleaned_Review'] = text_prep_main\n",
    "\n",
    "sample_df.to_csv('cleaned_selected_label_samples_withothers.csv', index=False)\n",
    "main_df.to_csv('cleaned_filtered_main.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    AlbertTokenizer, AlbertForSequenceClassification,\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    ElectraTokenizer, ElectraForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load datasets\n",
    "selected_label_samples = pd.read_csv('cleaned_selected_label_samples_withothers.csv')\n",
    "\n",
    "# Extract the necessary columns\n",
    "selected_texts = selected_label_samples['Cleaned_Review'].fillna('').tolist()\n",
    "\n",
    "# Create multi-label targets\n",
    "label_columns = ['Electric Fit', 'Affordability', 'Customer Care', 'Other']\n",
    "labels = selected_label_samples[label_columns].fillna('').values.tolist()\n",
    "labels = [list(filter(lambda x: x != '', label)) for label in labels]\n",
    "\n",
    "# Binarize the labels\n",
    "mlb = MultiLabelBinarizer(classes=label_columns)\n",
    "labels = mlb.fit_transform(labels)\n",
    "\n",
    "# Check if there are any issues with individual texts or labels\n",
    "for i, text in enumerate(selected_texts):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Non-string text found at index {i}: {text}\")\n",
    "for i, label in enumerate(labels):\n",
    "    if not isinstance(label, (list, tuple)):\n",
    "        print(f\"Non-list/tuple label found at index {i}: {label}\")\n",
    "\n",
    "# Check and remove any unintended labels\n",
    "intended_labels = label_columns  # the intended labels\n",
    "unwanted_labels = set(mlb.classes_) - set(intended_labels)\n",
    "\n",
    "if unwanted_labels:\n",
    "    print(f\"Unwanted labels detected: {unwanted_labels}\")\n",
    "    for label in unwanted_labels:\n",
    "        # Find the index of the unwanted label\n",
    "        index = list(mlb.classes_).index(label)\n",
    "        # Remove the column from labels\n",
    "        labels = np.delete(labels, index, axis=1)\n",
    "        # Remove the label from mlb.classes_\n",
    "        mlb.classes_ = np.delete(mlb.classes_, index)\n",
    "\n",
    "# Verify the cleaned labels\n",
    "print(\"Final labels:\", mlb.classes_)\n",
    "print(\"Sample labels after cleaning:\", labels[:5])\n",
    "\n",
    "# Function to encode data\n",
    "def encode_data(texts, tokenizer, labels=None, max_length=512):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    dataset = CustomDataset(encodings, labels)\n",
    "    return dataset\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(selected_texts, labels, test_size=0.2)\n",
    "\n",
    "# Dictionary of pre-trained model architectures and their corresponding tokenizers and model classes\n",
    "architectures = {\n",
    "    \"ALBERT\": (\"albert-base-v2\", AlbertTokenizer, AlbertForSequenceClassification),\n",
    "    \"BERT\": (\"bert-base-uncased\", BertTokenizer, BertForSequenceClassification),\n",
    "    \"DistilBERT\": (\"distilbert-base-uncased\", DistilBertTokenizer, DistilBertForSequenceClassification),\n",
    "    \"ELECTRA\": (\"google/electra-base-discriminator\", ElectraTokenizer, ElectraForSequenceClassification),\n",
    "    \"RoBERTa\": (\"roberta-base\", RobertaTokenizer, RobertaForSequenceClassification)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate_model(name, model_name, tokenizer_class, model_class):\n",
    "    print(f\"Training {name} model...\")\n",
    "\n",
    "    # Load pre-trained tokenizer and model\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name, num_labels=len(mlb.classes_))\n",
    "\n",
    "    # Encode datasets\n",
    "    train_dataset = encode_data(train_texts, tokenizer, train_labels)\n",
    "    val_dataset = encode_data(val_texts, tokenizer, val_labels)\n",
    "\n",
    "    # Set up training arguments and trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{name}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs/{name}',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the trained model and tokenizer\n",
    "    model.save_pretrained(f'./{name.lower()}_model')\n",
    "    tokenizer.save_pretrained(f'./{name.lower()}_tokenizer')\n",
    "\n",
    "    # Evaluate the model\n",
    "    trainer.evaluate()\n",
    "\n",
    "    # Get predictions on the validation set\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    preds = torch.sigmoid(torch.tensor(predictions.predictions)).round().numpy()\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    # Compute overall metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    roc_auc = roc_auc_score(labels, preds, average='weighted')\n",
    "    tn, fp, fn, tp = confusion_matrix(labels.ravel(), preds.ravel()).ravel()\n",
    "\n",
    "    # Collect overall metrics\n",
    "    overall_metrics = [name, labels.shape[0], tp, fn, fp, tn, accuracy, roc_auc, precision, recall, f1]\n",
    "\n",
    "    # Compute per-label metrics\n",
    "    precision_per_label, recall_per_label, f1_per_label, _ = precision_recall_fscore_support(labels, preds, average=None, labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Collect per-label metrics\n",
    "    per_label_metrics = []\n",
    "    for i, label in enumerate(label_columns):\n",
    "        per_label_metrics.append([name, label, precision_per_label[i], recall_per_label[i], f1_per_label[i]])\n",
    "\n",
    "    print(f'{name} - Accuracy: {accuracy}')\n",
    "    print(f'{name} - Precision: {precision}')\n",
    "    print(f'{name} - Recall: {recall}')\n",
    "    print(f'{name} - F1 Score: {f1}')\n",
    "    print(f'{name} - ROC AUC: {roc_auc}')\n",
    "\n",
    "    for i, label in enumerate(label_columns):\n",
    "        print(f'{name} - {label} - Precision: {precision_per_label[i]}')\n",
    "        print(f'{name} - {label} - Recall: {recall_per_label[i]}')\n",
    "        print(f'{name} - {label} - F1 Score: {f1_per_label[i]}')\n",
    "\n",
    "    # Return the results for this model\n",
    "    return overall_metrics, per_label_metrics\n",
    "\n",
    "# Train and evaluate each model\n",
    "overall_results = []\n",
    "label_results = []\n",
    "\n",
    "for name, (model_name, tokenizer_class, model_class) in architectures.items():\n",
    "    overall_metrics, per_label_metrics = train_and_evaluate_model(name, model_name, tokenizer_class, model_class)\n",
    "    overall_results.append(overall_metrics)\n",
    "    label_results.extend(per_label_metrics)\n",
    "\n",
    "# Convert lists to dataframes\n",
    "def process_and_save_overall_results(results):\n",
    "    columns = [\"Name\", \"Share\", \"True-Positives\", \"False-Negatives\", \"False-Positives\", \n",
    "               \"True-Negatives\", \"Accuracy\", \"AUC\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "    overall_results_table = pd.DataFrame(results, columns=columns)\n",
    "    overall_results_table[\"Type\"] = \"Transformer\"\n",
    "    overall_results_table = overall_results_table[[\"Name\", \"Type\", \"Share\", \"True-Positives\", \n",
    "                                                   \"False-Negatives\", \"False-Positives\", \n",
    "                                                   \"True-Negatives\", \"Accuracy\", \"AUC\", \n",
    "                                                   \"Precision\", \"Recall\", \"F1\"]]\n",
    "\n",
    "    # Output results\n",
    "    overall_results_table.sort_values(\"Accuracy\", ascending=False).to_csv(\"Transformer Overall Model Performance.csv\", index=False)\n",
    "    print(overall_results_table.sort_values(\"Accuracy\", ascending=False))\n",
    "\n",
    "def process_and_save_label_results(results):\n",
    "    columns = [\"Name\", \"Label\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    label_results_table = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    # Output results\n",
    "    label_results_table.to_csv(\"Transformer Per-Label Model Performance.csv\", index=False)\n",
    "    print(label_results_table)\n",
    "\n",
    "# Process and save the results\n",
    "process_and_save_overall_results(overall_results)\n",
    "process_and_save_label_results(label_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 apply selected model to annotate the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model and tokenizer from the specified directory\n",
    "model_path = './roberta_model'\n",
    "tokenizer_path = './roberta_tokenizer'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Move model to device (CPU or MPS)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the cleaned unlabelled main dataset\n",
    "cleaned_speakev = pd.read_csv('cleaned_filtered_main_removed_missing_duplicates.csv')\n",
    "\n",
    "# Ensure all texts in cleaned_speakev are strings\n",
    "cleaned_speakev_texts = [str(text) for text in cleaned_speakev['Cleaned_Review'].tolist() if isinstance(text, (str, int, float))]\n",
    "\n",
    "# Log the length of the texts to identify potential issues\n",
    "text_lengths = [len(text) for text in cleaned_speakev_texts]\n",
    "print(f\"Text length stats - Min: {min(text_lengths)}, Max: {max(text_lengths)}, Average: {sum(text_lengths)/len(text_lengths)}\")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Process the texts in batches\n",
    "for i in range(0, len(cleaned_speakev_texts), batch_size):\n",
    "    batch_texts = cleaned_speakev_texts[i:i + batch_size]\n",
    "    \n",
    "    # Tokenize the current batch of texts\n",
    "    cleaned_speakev_encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    # Move tensors to device\n",
    "    input_ids = cleaned_speakev_encodings['input_ids'].to(device)\n",
    "    attention_mask = cleaned_speakev_encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # Verify the shape of the encoded inputs\n",
    "    print(f\"Processing batch {i//batch_size + 1} / {len(cleaned_speakev_texts)//batch_size + 1}\")\n",
    "    print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.sigmoid(outputs.logits).round().tolist()\n",
    "            all_predictions.extend(predictions)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError: {e}\")\n",
    "            # Additional debugging or logging can be added here\n",
    "            raise\n",
    "\n",
    "# Convert all_predictions to a NumPy array\n",
    "all_predictions_array = np.array(all_predictions)\n",
    "\n",
    "# Ensure the shape of the array is correct\n",
    "if all_predictions_array.ndim == 1:\n",
    "    all_predictions_array = all_predictions_array.reshape(-1, 1)\n",
    "\n",
    "# Define the label columns\n",
    "label_columns = ['Electric Fit', 'Affordability', 'Customer Care', 'Other']\n",
    "\n",
    "# Load the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=label_columns)\n",
    "mlb.fit([label_columns])  # Fit the MultiLabelBinarizer with label columns\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = mlb.inverse_transform(all_predictions_array)\n",
    "\n",
    "# Add predictions to the cleaned_speakev dataframe\n",
    "cleaned_speakev['Predicted_Label'] = predicted_labels\n",
    "\n",
    "# Save the cleaned_speakev dataframe with predictions\n",
    "cleaned_speakev.to_csv('cleaned_filtered_main_removed_missing_duplicates_with_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions added and saved to 'main_with_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('/Users/janekao/Desktop/en_core_web_sm-3.1.0/en_core_web_sm/en_core_web_sm-3.1.0')\n",
    "\n",
    "# Register and add the TextBlob component\n",
    "spacy_text_blob = SpacyTextBlob(nlp)\n",
    "nlp.add_pipe(\"spacytextblob\", last=True)\n",
    "\n",
    "# Function to process reviews based on a specific label and perform sentiment analysis\n",
    "def process_reviews_by_label(df, label, output_filename):\n",
    "    # Filter reviews where the label is present in the 'Predicted_Label'\n",
    "    filtered_reviews = df[df['Predicted_Label'].apply(lambda x: label in x)]\n",
    "    \n",
    "    # Perform Sentiment Analysis using Cleaned_Review column with SpaCy and spacytextblob\n",
    "    def get_sentiment(text):\n",
    "        doc = nlp(text)\n",
    "        if doc._.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif doc._.polarity < 0:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    filtered_reviews['Sentiment'] = filtered_reviews['Cleaned_Review'].apply(get_sentiment)\n",
    "\n",
    "    # Aggregate results by car brand and sentiment\n",
    "    sentiment_summary = filtered_reviews.groupby(['Brand', 'Sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Add a 'Total' column to summarize the number of reviews for each sentiment category\n",
    "    sentiment_summary['Total'] = sentiment_summary.sum(axis=1)\n",
    "    \n",
    "    # Save the filtered reviews with sentiment analysis to a new CSV file\n",
    "    filtered_reviews.to_csv(output_filename, index=False)\n",
    "\n",
    "    # Display the path to the saved dataset\n",
    "    print(f\"Dataset with sentiments saved to: {output_filename}\")\n",
    "    return sentiment_summary\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('final.csv')\n",
    "\n",
    "# Process reviews for different labels\n",
    "labels = ['Electric Fit', 'Affordability', 'Customer Care']\n",
    "output_files = ['electric_fit.csv', 'affordability.csv', 'customer_care.csv']\n",
    "\n",
    "for label, output_file in zip(labels, output_files):\n",
    "    sentiment_summary = process_reviews_by_label(df, label, output_file)\n",
    "    print(f\"\\nSentiment summary for {label}:\")\n",
    "    print(sentiment_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Load the datasets\n",
    "electric_fit_df = pd.read_csv('electric_fit.csv')\n",
    "customer_care_df = pd.read_csv('customer_care.csv')\n",
    "affordability_df = pd.read_csv('affordability.csv')\n",
    "\n",
    "# Function to visualize sentiment distribution\n",
    "def visualize_sentiment_distribution(df, kpi_name):\n",
    "    sentiment_summary = df.groupby(['Brand', 'Sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_summary.plot(kind='bar', stacked=True, figsize=(10, 6), title=f'Sentiment Distribution for {kpi_name}')\n",
    "    plt.xlabel('Brand')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize each KPI\n",
    "visualize_sentiment_distribution(electric_fit_df, 'Electric Fit')\n",
    "visualize_sentiment_distribution(customer_care_df, 'Customer Care')\n",
    "visualize_sentiment_distribution(affordability_df, 'Affordability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 inital attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from pyLDAvis import display\n",
    "\n",
    "# Function to split KPIs from the Predicted_Label column\n",
    "def split_kpis(predicted_label):\n",
    "    return predicted_label.strip(\"()\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "# Apply the split function to create a KPI column\n",
    "df['KPI'] = df['Predicted_Label'].apply(split_kpis)\n",
    "\n",
    "# Explode the KPI column to have one KPI per row\n",
    "df_exploded = df.explode('KPI')\n",
    "\n",
    "# Exclude the \"Other\" label from analysis\n",
    "df_exploded = df_exploded[df_exploded['KPI'] != 'Other']\n",
    "\n",
    "# Split the data into Fiat and competitors\n",
    "fiat_df = df_exploded[df_exploded['Brand'] == 'Fiat']\n",
    "competitors_df = df_exploded[df_exploded['Brand'] != 'Fiat']\n",
    "\n",
    "print(f\"Fiat dataset contains {len(fiat_df)} reviews.\")\n",
    "print(f\"Competitors dataset contains {len(competitors_df)} reviews.\")\n",
    "\n",
    "# Build the dataset (corpus) for Fiat\n",
    "fiat_reviews = fiat_df['Cleaned_Review'].tolist()\n",
    "fiat_texts = [review.split() for review in fiat_reviews]  # Tokenize the cleaned reviews\n",
    "fiat_id2word = Dictionary(fiat_texts)\n",
    "fiat_corpus = [fiat_id2word.doc2bow(text) for text in fiat_texts]\n",
    "print('Fiat corpus contains {} reviews with {} unique words'.format(len(fiat_corpus), len(fiat_id2word)))\n",
    "\n",
    "# Train the topic model for Fiat\n",
    "num_topics = 15  # Number of topics\n",
    "fiat_lda_model = LdaModel(fiat_corpus, num_topics=num_topics, id2word=fiat_id2word, passes=15)\n",
    "\n",
    "# Print all topics for Fiat\n",
    "print(\"Fiat LDA Model:\")\n",
    "for idx, topic in fiat_lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Build the dataset (corpus) for Competitors\n",
    "competitors_reviews = competitors_df['Cleaned_Review'].tolist()\n",
    "competitors_texts = [review.split() for review in competitors_reviews]  # Tokenize the cleaned reviews\n",
    "competitors_id2word = Dictionary(competitors_texts)\n",
    "competitors_corpus = [competitors_id2word.doc2bow(text) for text in competitors_texts]\n",
    "print('Competitors corpus contains {} reviews with {} unique words'.format(len(competitors_corpus), len(competitors_id2word)))\n",
    "\n",
    "# Train the topic model for Competitors\n",
    "competitors_lda_model = LdaModel(competitors_corpus, num_topics=num_topics, id2word=competitors_id2word, passes=15)\n",
    "\n",
    "# Print all topics for Competitors\n",
    "print(\"\\nCompetitors LDA Model:\")\n",
    "for idx, topic in competitors_lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis for Fiat\n",
    "fiat_lda_vis = gensimvis.prepare(fiat_lda_model, fiat_corpus, fiat_id2word)\n",
    "display(fiat_lda_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis for Competitors\n",
    "competitors_lda_vis = gensimvis.prepare(competitors_lda_model, competitors_corpus, competitors_id2word)\n",
    "display(competitors_lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract topics from reviews\n",
    "def extract_topics(reviews, model, id2word, num_topics):\n",
    "    topics = []\n",
    "    for review in reviews:\n",
    "        bow = id2word.doc2bow(review.split())\n",
    "        doc_topics = model.get_document_topics(bow)\n",
    "        topic_vector = [0] * num_topics\n",
    "        for topic_num, score in doc_topics:\n",
    "            topic_vector[topic_num] = score\n",
    "        topics.append(topic_vector)\n",
    "    return topics\n",
    "\n",
    "# Extract topics for Fiat\n",
    "fiat_df['Topics'] = extract_topics(fiat_reviews, fiat_lda_model, fiat_id2word, num_topics)\n",
    "\n",
    "# Extract topics for Competitors\n",
    "competitors_df['Topics'] = extract_topics(competitors_reviews, competitors_lda_model, competitors_id2word, num_topics)\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column for Fiat\n",
    "fiat_topics_df = pd.DataFrame(fiat_df['Topics'].tolist(), columns=['Fiat_Topic_' + str(i) for i in range(num_topics)])\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column for Competitors\n",
    "competitors_topics_df = pd.DataFrame(competitors_df['Topics'].tolist(), columns=['Competitors_Topic_' + str(i) for i in range(num_topics)])\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame for Fiat\n",
    "fiat_df = pd.concat([fiat_df.reset_index(drop=True), fiat_topics_df], axis=1)\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame for Competitors\n",
    "competitors_df = pd.concat([competitors_df.reset_index(drop=True), competitors_topics_df], axis=1)\n",
    "\n",
    "# Ensure that the topic columns are numeric\n",
    "fiat_topic_columns = ['Fiat_Topic_' + str(i) for i in range(num_topics)]\n",
    "competitors_topic_columns = ['Competitors_Topic_' + str(i) for i in range(num_topics)]\n",
    "\n",
    "fiat_df[fiat_topic_columns] = fiat_df[fiat_topic_columns].apply(pd.to_numeric)\n",
    "competitors_df[competitors_topic_columns] = competitors_df[competitors_topic_columns].apply(pd.to_numeric)\n",
    "\n",
    "# Calculate average topic distributions for Fiat and Competitors\n",
    "fiat_averages = fiat_df[fiat_topic_columns].mean()\n",
    "competitors_averages = competitors_df[competitors_topic_columns].mean()\n",
    "\n",
    "print(f\"Fiat Average Topic Distributions:\\n{fiat_averages}\")\n",
    "print(f\"Competitors Average Topic Distributions:\\n{competitors_averages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 adjust number of topics : Fiat 11 & competitors 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Function to split KPIs from the Predicted_Label column\n",
    "def split_kpis(predicted_label):\n",
    "    return predicted_label.strip(\"()\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "# Apply the split function to create a KPI column\n",
    "df['KPI'] = df['Predicted_Label'].apply(split_kpis)\n",
    "\n",
    "# Explode the KPI column to have one KPI per row\n",
    "df_exploded = df.explode('KPI')\n",
    "\n",
    "# Exclude the \"Other\" label from analysis\n",
    "df_exploded = df_exploded[df_exploded['KPI'] != 'Other']\n",
    "\n",
    "# Split the data into Fiat and competitors\n",
    "fiat_df = df_exploded[df_exploded['Brand'] == 'Fiat']\n",
    "competitors_df = df_exploded[df_exploded['Brand'] != 'Fiat']\n",
    "\n",
    "# Build the dataset (corpus) for Fiat\n",
    "fiat_reviews = fiat_df['Cleaned_Review'].tolist()\n",
    "fiat_texts = [review.split() for review in fiat_reviews]  # Tokenize the cleaned reviews\n",
    "fiat_id2word = Dictionary(fiat_texts)\n",
    "fiat_corpus = [fiat_id2word.doc2bow(text) for text in fiat_texts]\n",
    "print('Fiat corpus contains {} reviews with {} unique words'.format(len(fiat_corpus), len(fiat_id2word)))\n",
    "\n",
    "# Adjust the number of topics\n",
    "num_topics_fiat = 10  # Reduced from 15 to 10\n",
    "\n",
    "# Train the topic model for Fiat\n",
    "fiat_lda_model = LdaModel(fiat_corpus, num_topics=num_topics_fiat, id2word=fiat_id2word, passes=15)\n",
    "\n",
    "# Print all topics for Fiat\n",
    "print(\"Fiat LDA Model:\")\n",
    "for idx, topic in fiat_lda_model.print_topics(num_topics=num_topics_fiat, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Build the dataset (corpus) for Competitors\n",
    "competitors_reviews = competitors_df['Cleaned_Review'].tolist()\n",
    "competitors_texts = [review.split() for review in competitors_reviews]  # Tokenize the cleaned reviews\n",
    "competitors_id2word = Dictionary(competitors_texts)\n",
    "competitors_corpus = [competitors_id2word.doc2bow(text) for text in competitors_texts]\n",
    "print('Competitors corpus contains {} reviews with {} unique words'.format(len(competitors_corpus), len(competitors_id2word)))\n",
    "\n",
    "# Adjust the number of topics\n",
    "num_topics_competitors = 12  # Reduced from 15 to 12\n",
    "\n",
    "# Train the topic model for Competitors\n",
    "competitors_lda_model = LdaModel(competitors_corpus, num_topics=num_topics_competitors, id2word=competitors_id2word, passes=15)\n",
    "\n",
    "# Print all topics for Competitors\n",
    "print(\"\\nCompetitors LDA Model:\")\n",
    "for idx, topic in competitors_lda_model.print_topics(num_topics=num_topics_competitors, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Function to extract topics from reviews\n",
    "def extract_topics(reviews, model, id2word, num_topics):\n",
    "    topics = []\n",
    "    for review in reviews:\n",
    "        bow = id2word.doc2bow(review.split())\n",
    "        doc_topics = model.get_document_topics(bow)\n",
    "        topic_vector = [0] * num_topics\n",
    "        for topic_num, score in doc_topics:\n",
    "            topic_vector[topic_num] = score\n",
    "        topics.append(topic_vector)\n",
    "    return topics\n",
    "\n",
    "# Extract topics for Fiat\n",
    "fiat_df['Topics'] = extract_topics(fiat_reviews, fiat_lda_model, fiat_id2word, num_topics_fiat)\n",
    "\n",
    "# Extract topics for Competitors\n",
    "competitors_df['Topics'] = extract_topics(competitors_reviews, competitors_lda_model, competitors_id2word, num_topics_competitors)\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column for Fiat\n",
    "fiat_topics_df = pd.DataFrame(fiat_df['Topics'].tolist(), columns=['Fiat_Topic_' + str(i) for i in range(num_topics_fiat)])\n",
    "\n",
    "# Convert 'Topics' column to a DataFrame with each topic as a separate column for Competitors\n",
    "competitors_topics_df = pd.DataFrame(competitors_df['Topics'].tolist(), columns=['Competitors_Topic_' + str(i) for i in range(num_topics_competitors)])\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame for Fiat\n",
    "fiat_df = pd.concat([fiat_df.reset_index(drop=True), fiat_topics_df], axis=1)\n",
    "\n",
    "# Concatenate the topics DataFrame with the original DataFrame for Competitors\n",
    "competitors_df = pd.concat([competitors_df.reset_index(drop=True), competitors_topics_df], axis=1)\n",
    "\n",
    "# Ensure that the topic columns are numeric\n",
    "fiat_topic_columns = ['Fiat_Topic_' + str(i) for i in range(num_topics_fiat)]\n",
    "competitors_topic_columns = ['Competitors_Topic_' + str(i) for i in range(num_topics_competitors)]\n",
    "\n",
    "fiat_df[fiat_topic_columns] = fiat_df[fiat_topic_columns].apply(pd.to_numeric)\n",
    "competitors_df[competitors_topic_columns] = competitors_df[competitors_topic_columns].apply(pd.to_numeric)\n",
    "\n",
    "# Calculate average topic distributions for Fiat and Competitors\n",
    "fiat_averages = fiat_df[fiat_topic_columns].mean()\n",
    "competitors_averages = competitors_df[competitors_topic_columns].mean()\n",
    "\n",
    "print(f\"Fiat Average Topic Distributions:\\n{fiat_averages}\")\n",
    "print(f\"Competitors Average Topic Distributions:\\n{competitors_averages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis for Fiat\n",
    "fiat_lda_vis = gensimvis.prepare(fiat_lda_model, fiat_corpus, fiat_id2word)\n",
    "display(fiat_lda_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis for Competitors\n",
    "competitors_lda_vis = gensimvis.prepare(competitors_lda_model, competitors_corpus, competitors_id2word)\n",
    "display(competitors_lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 explore topic 2 in fiat and topic 10 in competitors since they are dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Function to filter reviews based on dominant topic\n",
    "def filter_reviews_by_topic(reviews, model, id2word, topic_num, threshold=0.2):\n",
    "    filtered_reviews = []\n",
    "    for review in reviews:\n",
    "        bow = id2word.doc2bow(review.split())\n",
    "        doc_topics = model.get_document_topics(bow)\n",
    "        if any(topic == topic_num and score >= threshold for topic, score in doc_topics):\n",
    "            filtered_reviews.append(review)\n",
    "    return filtered_reviews\n",
    "\n",
    "# Step 1: Filter reviews for Fiat's dominant Topic 2\n",
    "fiat_topic_2_reviews = filter_reviews_by_topic(fiat_reviews, fiat_lda_model, fiat_id2word, topic_num=2, threshold=0.2)\n",
    "\n",
    "# Step 2: Create a new LDA model focused on these filtered reviews\n",
    "fiat_topic_2_texts = [review.split() for review in fiat_topic_2_reviews]\n",
    "fiat_topic_2_id2word = Dictionary(fiat_topic_2_texts)\n",
    "fiat_topic_2_corpus = [fiat_topic_2_id2word.doc2bow(text) for text in fiat_topic_2_texts]\n",
    "\n",
    "# Train a new LDA model with more topics, say 5-7\n",
    "fiat_refined_lda_model = LdaModel(fiat_topic_2_corpus, num_topics=5, id2word=fiat_topic_2_id2word, passes=15)\n",
    "\n",
    "# Print the refined topics for Fiat's Topic 2\n",
    "print(\"Refined Fiat LDA Model for Topic 2:\")\n",
    "for idx, topic in fiat_refined_lda_model.print_topics(num_topics=5, num_words=10):\n",
    "    print(f\"Refined Topic {idx}: {topic}\")\n",
    "\n",
    "# Step 3: Repeat the process for Competitors' dominant Topic 10\n",
    "competitors_topic_10_reviews = filter_reviews_by_topic(competitors_reviews, competitors_lda_model, competitors_id2word, topic_num=10, threshold=0.2)\n",
    "\n",
    "competitors_topic_10_texts = [review.split() for review in competitors_topic_10_reviews]\n",
    "competitors_topic_10_id2word = Dictionary(competitors_topic_10_texts)\n",
    "competitors_topic_10_corpus = [competitors_topic_10_id2word.doc2bow(text) for text in competitors_topic_10_texts]\n",
    "\n",
    "# Train a new LDA model with more topics, say 5-7\n",
    "competitors_refined_lda_model = LdaModel(competitors_topic_10_corpus, num_topics=5, id2word=competitors_topic_10_id2word, passes=15)\n",
    "\n",
    "# Print the refined topics for Competitors' Topic 10\n",
    "print(\"Refined Competitors LDA Model for Topic 10:\")\n",
    "for idx, topic in competitors_refined_lda_model.print_topics(num_topics=5, num_words=10):\n",
    "    print(f\"Refined Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Create the visualization for the refined topics\n",
    "vis_fiat_refined = gensimvis.prepare(fiat_refined_lda_model, fiat_topic_2_corpus, fiat_topic_2_id2word)\n",
    "\n",
    "# Display the refined topic visualization\n",
    "pyLDAvis.display(vis_fiat_refined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Create the visualization for the refined topics\n",
    "vis_competitors_refined = gensimvis.prepare(competitors_refined_lda_model, competitors_topic_10_corpus, competitors_topic_10_id2word)\n",
    "\n",
    "# Display the refined topic visualization\n",
    "pyLDAvis.display(vis_competitors_refined)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
